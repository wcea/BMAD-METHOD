# Story 1.9: Architectural Gaps Detected by Codex

## Status
Draft

## Story Statement
As the engineering team, we need to address the architectural gaps identified during the Codex review so the video reel creation engine can meet the technical requirements for scale, reliability, and feature completeness.

## Acceptance Criteria
1. Database and migrations: PostgreSQL is the default runtime database; Alembic migrations exist and are applied for all required schemas (projects/exports/analytics/clips and supporting indexes) with async session management wired through the services.
2. Storage abstraction: Pluggable storage service supports local, S3, and Azure with signed upload/download URLs, bucket/folder layout, retention/cleanup policies, and CDN readiness.
3. Task orchestration: Celery (or equivalent) provides multiple priority queues (ingest, analysis, generation, export), idempotent task envelopes with retry/backoff, dead-letter handling, and resource-hinted worker pools (CPU vs GPU).
4. Service boundaries: Components are split into deployable services (API gateway, ingest/uploader, analysis, clip generator/exporter, WebSocket notifier) with manifests/Helm values for independent scaling.
5. Processing pipeline resilience: FFmpeg/OpenCV workflows are chunked and checkpointed with resumable jobs, per-stage state persisted in DB/Redis, and backpressure when queues exceed thresholds.
6. GPU/ML service layer: Dedicated inference service with model registry/versioning, warmup, dynamic batching, mixed precision, GPU scheduling/auto-scaling, and A/B capability.
7. Export pipeline completeness: Platform-aware renderers with presets (safe zones, aspect ratios, quality levels), prioritized export queue, resumable exports, and time/size estimates.
8. Real-time updates: WebSocket/SSE event schema for upload/analysis/generation/export progress with room scoping and reconnect resilience; events persisted for missed updates.
9. Observability: OpenTelemetry tracing, Prometheus metrics per stage (latency, queue depth, GPU/CPU/mem), structured JSON logs with correlation IDs, and dashboards/alerts for SLOs (processing time, error rate, throughput).
10. Configuration and secrets: Centralized config without hard-coded secrets; environment/secret manager integration; startup validation that fails fast on missing critical settings.
11. Testing and performance gates: Integration/end-to-end/load tests for full pipeline, vertical/caption flows (Story 1.7), transcript fallback (Story 1.8), and media golden outputs; automated perf/load targets (50+ concurrent jobs) in CI.

## Tasks
- Database & migrations: Switch default URL to Postgres, add/verify Alembic migrations for projects/exports/analytics/clips, and standardize async DB sessions.
- Storage: Implement provider interface with S3/Azure clients, presigned URL helpers, bucket layout, cleanup, and CDN configuration points.
- Orchestration: Define queues, routing keys, idempotent task wrapper, retry/backoff/DLQ policies, and worker pool configs (CPU/GPU).
- Service decomposition: Extract API, ingest/uploader, analysis, generation/export, and notifier services; create deployment manifests/Helm values.
- Pipeline resilience: Add chunked processing, checkpoints, resumable logic, and backpressure signaling.
- GPU/ML layer: Stand up inference service with model registry, versioning, warmup, batching, mixed precision, and GPU scheduling.
- Export pipeline: Add platform presets, safe-zone handling, multi-aspect renders, prioritized queue, and resumable/export estimates.
- Real-time events: Define event schema, room scoping, persistence for missed updates, and reconnect handling.
- Observability: Add tracing, metrics, structured logs, correlation IDs, and dashboards/alerts for key SLOs.
- Config/secrets: Centralize configuration, remove hard-coded defaults, validate at startup, and integrate secret manager.
- Testing/perf: Add integration/E2E/load suites for pipeline, Story 1.7 vertical/caption, Story 1.8 transcript fallback, and enforce perf gates in CI.

## Technical Approach (Architecture Notes)
- Database: Promote Postgres as the primary DSN, add Alembic autogeneration with hand-tuned indexes, and refactor DB session usage to a single async session factory injected into services/workers; introduce migrations for projects/exports/analytics/clips with audit fields and partial indexes for status/queue lookups.
- Storage: Create a `StorageProvider` interface with adapters for Local/S3/Azure, plus presigned URL helpers and a consistent key layout (`org/{video_id}/raw|frames|clips|exports`); wire lifecycle policies/cleanup tasks and allow CDN fronting via config.
- Orchestration: Split Celery queues (`ingest`, `analysis`, `generation`, `export`) with routing keys; wrap tasks in an idempotent envelope (idempotency key, retry/backoff, DLQ routing, dedup); define worker pools per queue (CPU vs GPU) with concurrency caps and resource hints.
- Service boundaries: Package API gateway, ingest/uploader, analysis, clip generation/export, and notifier as separate deployables; add Helm values/manifests with horizontal pod autoscaling per service and shared config/secrets via env/secret manager.
- Pipeline resilience: Add chunked FFmpeg/OpenCV processing with stage checkpoints in DB/Redis; persist per-stage progress and allow resume; implement queue backpressure by rejecting/deferring tasks when queue depth/CPU/GPU thresholds are exceeded.
- GPU/ML layer: Introduce an inference service hosting models via a registry (MLflow-compatible), supporting model version tags, warmup, dynamic batching, mixed precision, GPU pinning, and A/B selection; API exposed over gRPC/HTTP with health/metrics.
- Export pipeline: Implement platform preset registry (TikTok/Reels/Shorts) with safe zones and aspect variants; exports run through a prioritized queue with resumable jobs, estimated time/size, and CRF/bitrate presets per quality tier.
- Real-time updates: Standardize event schema (`upload/analysis/generation/export`) with room scoping; persist last-known state so reconnecting clients can catch up; support WebSocket with optional SSE fallback and heartbeat/reconnect policy.
- Observability: Add OpenTelemetry tracing, Prometheus metrics per stage (latency, queue depth, GPU/CPU/mem), structured JSON logs with correlation/request/task IDs; ship dashboards and alert rules for SLOs (processing time, error rate, throughput, queue health).
- Config/secrets: Move all secrets to env/secret manager; remove hard-coded defaults; add startup validators that fail fast on missing critical config and output a readiness summary.
- Testing/perf: Add integration/E2E pipelines that exercise upload→analysis→clip generation→export (including Story 1.7 vertical/caption and Story 1.8 transcript fallback), plus load tests for 50+ concurrent jobs and golden-media comparisons; enforce perf gates in CI.

## Progress Updates
- AC #1 implemented: default runtime database moved to PostgreSQL (`postgresql+asyncpg`), sync URLs derived for Alembic/Celery, and async session factory standardized in `app/db/session.py`.

## Notes / Next Steps
- Keep `.env` pointing to `postgresql+asyncpg://postgres:postgres@localhost:5432/bmad_video_processor`.
- To rerun migrations (with Homebrew Postgres running): `set -a && source .env && set +a && venv/bin/alembic upgrade head`.
